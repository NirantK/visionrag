<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Vision RAG"><link href=https://visionrag.ai/blog/category/document-retrieval/ rel=canonical><link href=../colpali/ rel=prev><link href=../quick-notes/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.47"><title>Document Retrieval - VisionRAG</title><link rel=stylesheet href=../../../assets/stylesheets/main.6f8fc17f.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.75 1.75 0 0 1 1 7.775m1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2"/></svg>');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.5 1.75v11.5c0 .138.112.25.25.25h3.17a.75.75 0 0 1 0 1.5H2.75A1.75 1.75 0 0 1 1 13.25V1.75C1 .784 1.784 0 2.75 0h8.5C12.216 0 13 .784 13 1.75v7.736a.75.75 0 0 1-1.5 0V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25m13.274 9.537zl-4.557 4.45a.75.75 0 0 1-1.055-.008l-1.943-1.95a.75.75 0 0 1 1.062-1.058l1.419 1.425 4.026-3.932a.75.75 0 1 1 1.048 1.074M4.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5M4 7.75A.75.75 0 0 1 4.75 7h2a.75.75 0 0 1 0 1.5h-2A.75.75 0 0 1 4 7.75"/></svg>');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8m8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13M6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75M8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2"/></svg>');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M3.499.75a.75.75 0 0 1 1.5 0v.996C5.9 2.903 6.793 3.65 7.662 4.376l.24.202c-.036-.694.055-1.422.426-2.163C9.1.873 10.794-.045 12.622.26 14.408.558 16 1.94 16 4.25c0 1.278-.954 2.575-2.44 2.734l.146.508.065.22c.203.701.412 1.455.476 2.226.142 1.707-.4 3.03-1.487 3.898C11.714 14.671 10.27 15 8.75 15h-6a.75.75 0 0 1 0-1.5h1.376a4.5 4.5 0 0 1-.563-1.191 3.84 3.84 0 0 1-.05-2.063 4.65 4.65 0 0 1-2.025-.293.75.75 0 0 1 .525-1.406c1.357.507 2.376-.006 2.698-.318l.009-.01a.747.747 0 0 1 1.06 0 .75.75 0 0 1-.012 1.074c-.912.92-.992 1.835-.768 2.586.221.74.745 1.337 1.196 1.621H8.75c1.343 0 2.398-.296 3.074-.836.635-.507 1.036-1.31.928-2.602-.05-.603-.216-1.224-.422-1.93l-.064-.221c-.12-.407-.246-.84-.353-1.29a2.4 2.4 0 0 1-.507-.441 3.1 3.1 0 0 1-.633-1.248.75.75 0 0 1 1.455-.364c.046.185.144.436.31.627.146.168.353.305.712.305.738 0 1.25-.615 1.25-1.25 0-1.47-.95-2.315-2.123-2.51-1.172-.196-2.227.387-2.706 1.345-.46.92-.27 1.774.019 3.062l.042.19.01.05c.348.443.666.949.94 1.553a.75.75 0 1 1-1.365.62c-.553-1.217-1.32-1.94-2.3-2.768L6.7 5.527c-.814-.68-1.75-1.462-2.692-2.619a3.7 3.7 0 0 0-1.023.88c-.406.495-.663 1.036-.722 1.508.116.122.306.21.591.239.388.038.797-.06 1.032-.19a.75.75 0 0 1 .728 1.31c-.515.287-1.23.439-1.906.373-.682-.067-1.473-.38-1.879-1.193L.75 5.677V5.5c0-.984.48-1.94 1.077-2.664.46-.559 1.05-1.055 1.673-1.353z"/></svg>');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.75.75 0 0 1 .018-1.042.75.75 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0"/></svg>');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8m8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13M6.92 6.085h.001a.749.749 0 1 1-1.342-.67c.169-.339.436-.701.849-.977C6.845 4.16 7.369 4 8 4a2.76 2.76 0 0 1 1.637.525c.503.377.863.965.863 1.725 0 .448-.115.83-.329 1.15-.205.307-.47.513-.692.662-.109.072-.22.138-.313.195l-.006.004a6 6 0 0 0-.26.16 1 1 0 0 0-.276.245.75.75 0 0 1-1.248-.832c.184-.264.42-.489.692-.661q.154-.1.313-.195l.007-.004c.1-.061.182-.11.258-.161a1 1 0 0 0 .277-.245C8.96 6.514 9 6.427 9 6.25a.61.61 0 0 0-.262-.525A1.27 1.27 0 0 0 8 5.5c-.369 0-.595.09-.74.187a1 1 0 0 0-.34.398M9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0"/></svg>');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0M9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0"/></svg>');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.344 2.343za8 8 0 0 1 11.314 11.314A8.002 8.002 0 0 1 .234 10.089a8 8 0 0 1 2.11-7.746m1.06 10.253a6.5 6.5 0 1 0 9.108-9.275 6.5 6.5 0 0 0-9.108 9.275M6.03 4.97 8 6.94l1.97-1.97a.749.749 0 0 1 1.275.326.75.75 0 0 1-.215.734L9.06 8l1.97 1.97a.749.749 0 0 1-.326 1.275.75.75 0 0 1-.734-.215L8 9.06l-1.97 1.97a.749.749 0 0 1-1.275-.326.75.75 0 0 1 .215-.734L6.94 8 4.97 6.03a.75.75 0 0 1 .018-1.042.75.75 0 0 1 1.042-.018"/></svg>');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M9.504.43a1.516 1.516 0 0 1 2.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 0 1-.871.354h-.302a1.25 1.25 0 0 1-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004zm1.047 1.074L3.286 8.571A.25.25 0 0 0 3.462 9H6.75a.75.75 0 0 1 .694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0 0 12.538 7H9.25a.75.75 0 0 1-.683-1.06l2.008-4.418.003-.006-.004-.009-.006-.006-.008-.001q-.005 0-.009.004"/></svg>');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M4.72.22a.75.75 0 0 1 1.06 0l1 .999a3.5 3.5 0 0 1 2.441 0l.999-1a.748.748 0 0 1 1.265.332.75.75 0 0 1-.205.729l-.775.776c.616.63.995 1.493.995 2.444v.327q0 .15-.025.292c.408.14.764.392 1.029.722l1.968-.787a.75.75 0 0 1 .556 1.392L13 7.258V9h2.25a.75.75 0 0 1 0 1.5H13v.5q-.002.615-.141 1.186l2.17.868a.75.75 0 0 1-.557 1.392l-2.184-.873A5 5 0 0 1 8 16a5 5 0 0 1-4.288-2.427l-2.183.873a.75.75 0 0 1-.558-1.392l2.17-.868A5 5 0 0 1 3 11v-.5H.75a.75.75 0 0 1 0-1.5H3V7.258L.971 6.446a.75.75 0 0 1 .558-1.392l1.967.787c.265-.33.62-.583 1.03-.722a1.7 1.7 0 0 1-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72 1.28a.75.75 0 0 1 0-1.06m.53 6.28a.75.75 0 0 0-.75.75V11a3.5 3.5 0 1 0 7 0V7.25a.75.75 0 0 0-.75-.75ZM6.173 5h3.654A.17.17 0 0 0 10 4.827V4.5a2 2 0 1 0-4 0v.327c0 .096.077.173.173.173"/></svg>');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M5 5.782V2.5h-.25a.75.75 0 0 1 0-1.5h6.5a.75.75 0 0 1 0 1.5H11v3.282l3.666 5.76C15.619 13.04 14.543 15 12.767 15H3.233c-1.776 0-2.852-1.96-1.899-3.458Zm-2.4 6.565a.75.75 0 0 0 .633 1.153h9.534a.75.75 0 0 0 .633-1.153L12.225 10.5h-8.45ZM9.5 2.5h-3V6c0 .143-.04.283-.117.403L4.73 9h6.54L9.617 6.403A.75.75 0 0 1 9.5 6Z"/></svg>');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1.75 2.5h10.5a.75.75 0 0 1 0 1.5H1.75a.75.75 0 0 1 0-1.5m4 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5m0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5M2.5 7.75v6a.75.75 0 0 1-1.5 0v-6a.75.75 0 0 1 1.5 0"/></svg>');}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../assets/_mkdocstrings.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#document-retrieval class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title=VisionRAG class="md-header__button md-logo" aria-label=VisionRAG data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> VisionRAG </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Document Retrieval </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=deep-purple data-md-color-accent=amber aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Getting Started </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title=VisionRAG class="md-nav__button md-logo" aria-label=VisionRAG data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> VisionRAG </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Getting Started </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Blog </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3 checked> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=true> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../colpali/ class=md-nav__link> <span class=md-ellipsis> ColPali </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Document Retrieval </span> </a> </li> <li class=md-nav__item> <a href=../quick-notes/ class=md-nav__link> <span class=md-ellipsis> Quick Notes </span> </a> </li> <li class=md-nav__item> <a href=../rag/ class=md-nav__link> <span class=md-ellipsis> RAG </span> </a> </li> <li class=md-nav__item> <a href=../machine-learning/ class=md-nav__link> <span class=md-ellipsis> machine-learning </span> </a> </li> <li class=md-nav__item> <a href=../production/ class=md-nav__link> <span class=md-ellipsis> production </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <div class=md-content__inner> <header class=md-typeset> <h1 id=document-retrieval>Document Retrieval<a class=headerlink href=#document-retrieval title="Permanent link">&para;</a></h1> </header> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src="https://media.licdn.com/dms/image/v2/D5603AQEC2K_CA2dIOg/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1728967421425?e=1738195200&v=beta&t=kY1xEhj5NYWe0omU86hfCdWbk9hwxC0yQ671NOoyxs4" alt="Antaripa Saha"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2024-10-06 00:00:00+00:00">October 6, 2024</time></li> <li class=md-meta__item> in <a href=../colpali/ class=md-meta__link>ColPali</a>, <a href=./ class=md-meta__link>Document Retrieval</a></li> <li class=md-meta__item> 18 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=colpali-document-retrieval-with-vision-language-models><a href=../../colpali-indepth-blog/ class=toclink>ColPali: Document Retrieval with Vision Language Models ✨</a></h2> <p>🏹 We will be diving deep into the paper: <strong><a href=https://arxiv.org/pdf/2407.01449>[Arxiv] ColPali: Efficient Document Retrieval with Vision Language Models</a></strong></p> <p>Document retrieval has always been a key component of systems like search engines and information retrieval. Traditional document retrieval methods rely heavily on text-based methods (like OCR and text segmentation), often missing crucial visual cues like layouts, images, and tables.</p> <p>ColPali addresses this by using <strong>Vision-Language Models (VLMs)</strong> to understand and retrieve visually rich documents, capturing both textual and visual information. ColPali's architecture allows direct encoding of document images into a common embedding space, eliminating the need for time-consuming text extraction and segmentation.</p> <p><strong>In this blog, we’ll explore the technicalities behind ColPali covering the topics:</strong></p> <ul> <li><strong>Architecture of ColPali</strong></li> <li><strong>Training techniques</strong> <ul> <li><strong>Contrastive Loss</strong> for query-document matching.</li> <li>Training on <strong>127,460 query-document pairs</strong> (real + synthetic data).</li> </ul> </li> <li><strong>How it transforms embeddings using techniques like:</strong> <ul> <li><strong>BiSigLIP</strong> for visual-textual embeddings.</li> <li><strong>BiPali</strong> for pairing image patches with PaliGemma.</li> <li><strong>Late Interaction</strong> to achieve state-of-the-art performance.</li> </ul> </li> </ul> <p><img alt=image.png src=../../images/colpali_architecture_comparison.png></p> <hr> <p>Before we dive into the technical architecture and training of ColPali, let’s walk through the intuition behind how it works.</p> <h3 id=intuition-behind-colpali-how-it-simplifies-document-retrieval><a class=toclink href=../../colpali-indepth-blog/#intuition-behind-colpali-how-it-simplifies-document-retrieval><strong>Intuition Behind ColPali: How It Simplifies Document Retrieval</strong></a></h3> <h4 id=step-1-treating-the-document-as-an-image><a class=toclink href=../../colpali-indepth-blog/#step-1-treating-the-document-as-an-image>Step 1: Treating the Document as an Image</a></h4> <p>Imagine we have a PDF document. Normally, we would extract text from the document using OCR (Optical Character Recognition), segment it into different sections, and then use these segments for searching. ColPali simplifies this process by treating the entire document page as an image, bypassing the need for complex text extraction.</p> <ul> <li><strong>Think of it as taking a photograph of each page of the document.</strong> No need to convert the text, handle different languages, or worry about complex layouts. Just treat the page like a picture.</li> </ul> <h4 id=step-2-splitting-the-image-into-patches><a class=toclink href=../../colpali-indepth-blog/#step-2-splitting-the-image-into-patches>Step 2: Splitting the Image into Patches</a></h4> <p>Once ColPali has this "image" of the document, it divides the page into small, uniform pieces called <strong>patches (for eg, 16x16</strong> pixels).</p> <ul> <li><strong>Each patch captures a tiny portion of the page</strong>—it might contain a few words, a piece of a graph, or part of an image. This division helps the model to focus on small, detailed parts of the document rather than trying to understand the whole page at once.</li> <li> <p><strong>How are patches diff from chunking? Will it not lose context in between?</strong></p> <p>At first glance, it might seem like dividing an image into patches is similar to breaking text into chunks. However, there are several key differences between these two methods, especially in how they handle and preserve context. Let’s dive deeper into these differences to understand why patch-based processing in ColPali is more effective for document retrieval compared to traditional text chunking.</p> <h5 id=understanding-context-loss-in-text-chunking><a class=toclink href=../../colpali-indepth-blog/#understanding-context-loss-in-text-chunking>Understanding Context Loss in Text Chunking</a></h5> <p>In traditional text chunking, text is split into smaller chunks based on a certain number of tokens since many models have a limit on the number of tokens they can process at once.</p> <p><strong>Problem with Context Loss:</strong></p> <ul> <li>Chunking can split sentences or paragraphs mid-way, losing crucial contex and can result in incomplete information in one chunk and missing context in another.</li> <li>Chunking doesn't preserve visual or structural information, like the relationship between headings and their corresponding content or the placement of text in tables or figures.</li> </ul> <p><strong>Visual Example:</strong> If you have a document with a heading followed by a table, text chunking might separate the heading and the table, losing the context that the table belongs to that heading.</p> <h5 id=patch-based-image-processing-in-colpali><a class=toclink href=../../colpali-indepth-blog/#patch-based-image-processing-in-colpali>Patch-Based Image Processing in ColPali</a></h5> <p>ColPali divides the document image into patches, much like dividing a photo into small squares. Each patch is a fixed-size portion of the image, like a mini-snapshot of that part of the page.</p> <p><strong>Why Patches are More Effective:</strong></p> <p><strong>No Loss of Structure:</strong> The patches retain the visual structure of the document, preserving its spatial layout. For instance, if a page has two columns of text or a table with rows and columns, each patch maintains its relative position, ensuring that the model understands the overall arrangement of the elements.</p> <p><strong>Multi-Modal Context:</strong> Patches capture both textual and visual information. This includes both visual features (e.g., font styles, colors, boldness) and non-text elements (e.g., figures and graphs).</p> <p><strong>Positional Awareness:</strong> Each patch has a positional embedding that tells the model where it is located on the page, helping the model understand the overall layout.</p> </li> </ul> <h4 id=step-3-embedding-creation-and-aligning-visual-and-textual-information><a class=toclink href=../../colpali-indepth-blog/#step-3-embedding-creation-and-aligning-visual-and-textual-information>Step 3: Embedding Creation and <strong>Aligning Visual and Textual Information</strong></a></h4> <p>Each patch is then passed through a Vision Transformer (ViT), which converts them into unique <strong>embeddings</strong>—mathematical codes that represent the content within each patch.</p> <p>Next, ColPali aligns these visual embeddings with the text of the query by transforming the query into its own set of embeddings. ColPali uses a process called <strong>alignment,</strong> that aligns image path embeddings and text embeddings in the same vector space only then we can compare the similarity between query and document embeddings. </p> <h4 id=step-4-scoring-the-relevance-late-interaction-mechanism><a class=toclink href=../../colpali-indepth-blog/#step-4-scoring-the-relevance-late-interaction-mechanism>Step 4: Scoring the Relevance - Late Interaction Mechanism</a></h4> <p>At this point, ColPali has embeddings for both the query and the document. The next challenge is to identify the relevant parts of the document. ColPali uses a process called the <strong>Late Interaction Mechanism</strong>, where each piece of the query is finely matched against every part of the document, scoring and ranking their relevance.</p> <p>ColPali highlights the most relevant pieces of the document, focusing on the patches that best match the query. This approach enables ColPali to efficiently retrieve relevant information from visually rich documents, capturing both visual and textual data without losing context.</p> <hr> <p><strong>Let’s dive deeper now…</strong></p> <h3 id=colpali-architecture-a-detailed-exploration><a class=toclink href=../../colpali-indepth-blog/#colpali-architecture-a-detailed-exploration><strong>ColPali Architecture: A Detailed Exploration</strong></a></h3> <p>ColPali’s architecture is an innovative document retrieval system that handles visual data exclusively. ColPali directly processes document images, representing them as image patches. When a query (in textual form) is introduced, it is embedded into the same shared latent space as the image patches, allowing seamless comparison between the two.</p> <h4 id=1-vision-language-models-vlms-aligning-text-and-image-embeddings-in-shared-space><a class=toclink href=../../colpali-indepth-blog/#1-vision-language-models-vlms-aligning-text-and-image-embeddings-in-shared-space><strong>1. Vision-Language Models (VLMs)</strong>: Aligning Text and Image Embeddings in Shared Space</a></h4> <p>ColPali leverages <strong>Vision-Language Models (VLMs)</strong> to generate embeddings for both the visual (image patches) and textual content of documents. These image patches are treated as tokens, much like how words are treated in language models. The specific model extended by ColPali is <strong>PaliGemma-3B</strong>, which is known for its efficiency in processing visually rich documents.</p> <p><strong>PaliGemma-3B</strong> combines <strong>Vision Transformers (ViTs)</strong> for visual data with <strong>language models</strong> for textual data, creating a shared representation space for both. The vision transformer breaks down an input image into smaller patches (e.g., 16x16 pixels), and each patch is treated as a separate token. These tokens are then passed through the transformer, similar to how text tokens are processed.</p> <p><strong><em>Mathematical Intuition of Image Patch Embedding</em>:</strong></p> <ul> <li>Let <span class=arithmatex>\(x_{\text{img}} = \{ p_1, p_2, \dots, p_m \}\)</span> represent the sequence of image patches from a document, where each patch <span class=arithmatex>\(p_i\)</span> is a segment of the overall document image.</li> </ul> <p>The <strong>Vision Transformer (ViT)</strong> processes each patch independently and generates embeddings:</p> <div class=arithmatex>\[ E_{\text{img}} = \text{VisionEncoder}(x_{\text{img}}) = \{ e_1, e_2, \dots, e_m \} \]</div> <p>where <span class=arithmatex>\(E_{\text{img}} \in \mathbb{R}^{m \times D}\)</span> and <span class=arithmatex>\(D\)</span> is the dimensionality of the embedding space.</p> <p>These embeddings are then compared to the embeddings of the text query, all within the same shared space.</p> <hr> <p><strong>Text Query Embedding</strong>: When a query is provided, ColPali converts it into a sequence of tokens and passes these tokens through a language model. The same VLM architecture is employed to embed the text query into the same latent space as the image patches. This shared space ensures that the query embeddings can be directly compared with the document image embeddings.</p> <p><em>Mathematical Intuition</em>:</p> <ul> <li>Let <span class=arithmatex>\(x_{\text{query}} = \{ q_1, q_2, \dots, q_n \}\)</span> represent the query tokens, where n is the number of tokens in the query.</li> </ul> <p>The <strong>Text Encoder</strong> then embeds the query:</p> <div class=arithmatex>\[ E_{\text{query}} = \text{TextEncoder}(x_{\text{query}}) = \{ e'_1, e'_2, \dots, e'_n \} \]</div> <p>where <span class=arithmatex>\(E_{\text{query}} \in \mathbb{R}^{n \times D}\)</span></p> <p>This means that both the image patches and the text query are represented in the same <span class=arithmatex>\(D\)</span>-dimensional space, facilitating a direct comparison.</p> <hr> <p><strong>Let’s understand in detail the intuition behind alignment of text and image embeddings..</strong></p> <h3 id=aligning-text-and-image-embeddings-in-vlm><a class=toclink href=../../colpali-indepth-blog/#aligning-text-and-image-embeddings-in-vlm>Aligning Text and Image Embeddings in VLM</a></h3> <p>One of the most fascinating challenges in <strong>vision-language models</strong> is that it must handle two very different types of data: <strong>text</strong> and <strong>images</strong>. These two modalities not only come from different domains but also have <strong>different dimensionalities</strong>. For VLM to work effectively, it needs to <strong>align</strong> these different representations so that the text and image data can be compared in a <strong>common space</strong>.</p> <h4 id=the-problem-different-sizes-different-worlds><a class=toclink href=../../colpali-indepth-blog/#the-problem-different-sizes-different-worlds>The Problem: Different Sizes, Different Worlds</a></h4> <p>VLM model has the ability to <strong>compare</strong> text queries (like "Show me the sales report") with <strong>visual elements</strong> from a document, such as a table or chart. But the embeddings that represent text and image data needs to exist in the same dimensional space. Say,</p> <ul> <li><strong>Text Embeddings</strong>: Generated from transformer-based models (such as BERT, GPT), text embeddings typically have sizes like <strong>768 dimensions</strong> or <strong>1024 dimensions</strong>.</li> <li><strong>Image Embeddings</strong>: Produced from <strong>Vision Transformers (ViT)</strong>, image embeddings often have larger dimensions like <strong>1024</strong> or <strong>2048 dimensions</strong>. These embeddings encode the <strong>visual features</strong> of image patches.</li> </ul> <p>Now the question now is, <strong>how do you compare text embeddings of size 768 with image embeddings of size 1024?</strong> We can’t, since they are incompatible. </p> <h4 id=the-solution-linear-projections><a class=toclink href=../../colpali-indepth-blog/#the-solution-linear-projections>The Solution: Linear Projections</a></h4> <p><strong>Linear projection</strong> is a technique used to bring vectors from different spaces into a common latent space. Here’s the mathematical intuition behind it:</p> <p><strong>Mapping Text Embeddings and Image Embeddings to the Shared Space</strong></p> <p>Let’s say our <strong>text embedding</strong> is of size <strong>768 and</strong> image embedding of size <strong>1024</strong>. We want to transform this into a new space of size <strong>128 (this shared space dimension was used in colpali)</strong> so that it matches the dimensionality of the image embeddings.</p> <p>This is done using a <strong>linear projection</strong>:</p> <div class=arithmatex>\[ E^{\text{text}} = W_{text} \cdot E_{text} + b_{text} \]</div> <div class=arithmatex>\[ E^{\text{image}} = W_{image} \cdot E_{image} + b_{image} \]</div> <p>Where:</p> <ul> <li><span class=arithmatex>\(E_{text}\)</span> is your original text embedding (a vector of size 768).</li> <li><span class=arithmatex>\(W_{text} \in \mathbb{R}^{128 \times 768}\)</span> is the weight matrix that transforms the text embedding from dimension 768 to 128.</li> <li><span class=arithmatex>\(b_{text} \in \mathbb{R}^{128}\)</span> is a bias term that shifts the transformed vector appropriately.</li> <li><span class=arithmatex>\(E_{image}\)</span> is the original image embedding (a vector of size 1024).</li> <li><span class=arithmatex>\(W_{image} \in \mathbb{R}^{128 \times 1024}\)</span> is the weight matrix that transforms the image embeddings from dimension 1024 to 128.</li> <li><span class=arithmatex>\(b_{image} \in \mathbb{R}^{128}\)</span> is a bias term.</li> </ul> <p>This shared space enables cross-modal comparisons, meaning a textual query can retrieve relevant document images based on visual content, such as tables, charts, or other visual elements.</p> <p>🏹 <strong>Bonus: You might ask why 128 shared space dimension was used in Colpali? Any specific reason?</strong></p> <p>To maintain a balance between memory efficiency and computational cost, while minimizing performance drops, we chose a 128-dimensional embedding. This dimension will later be used in a multi-vector reranking process, so smaller is better, but it’s essential to strike the right balance for performance. Future versions are planned to offer the option to increase the embedding dimensions.</p> <hr> <h4 id=2-late-interaction-mechanism-for-finding-similarity-between-image-and-query><a class=toclink href=../../colpali-indepth-blog/#2-late-interaction-mechanism-for-finding-similarity-between-image-and-query><strong>2. Late Interaction Mechanism (for finding similarity between image and query)</strong></a></h4> <p>The <strong>Late Interaction</strong> mechanism in ColPali is designed to perform <strong>token-level similarity matching</strong> between the text query and document image patches. Unlike traditional retrieval models that reduce everything into a single embedding vector for comparison, <strong>Late Interaction</strong> operates on individual token embeddings, preserving granular details and improving accuracy in retrieval tasks.</p> <p>The key idea is that for each query token, ColPali finds the <strong>most relevant image patch</strong> in the document. The model then aggregates these relevance scores to compute the overall similarity between the query and the document.</p> <p><strong>Similarity Scoring with Late Interaction</strong></p> <p>Late Interaction computes the similarity between <strong>each query token</strong> and <strong>each image patch</strong>. For each query token, it identifies the most relevant image patch using <strong>MaxSim</strong> (maximum similarity). Finally, the total similarity score between the query and the document is the sum of the <strong>maximum similarities</strong> for all query tokens.</p> <h4 id=mathematical-intuition-behind-late-interaction><a class=toclink href=../../colpali-indepth-blog/#mathematical-intuition-behind-late-interaction><strong>Mathematical Intuition Behind Late Interaction:</strong></a></h4> <ul> <li><strong>Dot product:</strong> For each query token <span class=arithmatex>\(e_i′\)</span> compute its similarity with each image patch <span class=arithmatex>\(e_j\)</span> using the dot product. This gives a measure of how well query token <span class=arithmatex>\(e_i′\)</span> aligns with image patch <span class=arithmatex>\(e_j\)</span>.</li> </ul> <div class=arithmatex>\[ \text{Similarity}(e'_i, e_j) = e_i' \cdot e_j \]</div> <ul> <li><strong>MaxSim (Maximum Similarity)</strong>: For each query token <span class=arithmatex>\(e_i′\)</span> , find the <strong>most similar image patch</strong> using the maximum similarity across all image patches. This ensures that only the most relevant image patch is considered for each query token.</li> </ul> <div class=arithmatex>\[ \text{MaxSim}(e'_i, E_{\text{img}}) = \max_{j=1}^{m} \langle e'_i, e_j \rangle \]</div> <ul> <li><strong>Late Interaction Score</strong>: Finally, the total similarity score between the query and the document is computed by summing up the maximum similarities for all query tokens.</li> </ul> <div class=arithmatex>\[ LI(E_{\text{query}}, E_{\text{img}}) = \sum_{i=1}^{n} \text{MaxSim}(e'_i, E_{\text{img}}) \]</div> <p><img alt=image.png src=../../images/late_interaction.png></p> <p>The term <strong>Late Interaction</strong> refers to the idea that instead of merging or averaging embeddings from both the query and the document before comparison (as is often done in traditional dense retrieval methods), we <strong>retain token-level interactions</strong> until later in the retrieval process.</p> <hr> <h3 id=training-techniques-and-process-in-colpali><a class=toclink href=../../colpali-indepth-blog/#training-techniques-and-process-in-colpali><strong>Training Techniques and Process in ColPali</strong></a></h3> <p>Now we will deep-dive into how ColPali is trained to achieve its powerful document retrieval capabilities. The training process focuses on <strong>contrastive learning</strong>, multi-modal alignment (already covered), and the use of techniques like <strong>Low-Rank Adapters (LoRA)</strong>. The dataset is built from both <strong>real and synthetic data</strong>, further enhancing ColPali’s generalization capabilities. </p> <h4 id=1-contrastive-loss-for-query-document-matching><a class=toclink href=../../colpali-indepth-blog/#1-contrastive-loss-for-query-document-matching><strong>1. Contrastive Loss for Query-Document Matching</strong></a></h4> <p>Here, <strong>contrastive loss</strong> is used to train the model to correctly associate a <strong>query</strong> with its corresponding <strong>document page</strong> while distinguishing it from other irrelevant pages. The core idea behind constrastive loss is to <strong>pull together</strong> positive query-document pairs and <strong>push apart</strong> negative pairs in the embedding space, enabling the model to learn more discriminative representations for both the query and the document.</p> <h4 id=mathematical-intuition-of-contrastive-loss><a class=toclink href=../../colpali-indepth-blog/#mathematical-intuition-of-contrastive-loss><strong>Mathematical Intuition of Contrastive Loss</strong></a></h4> <p>Let’s define:</p> <ul> <li><strong>Positive Pair</strong>: A query <span class=arithmatex>\(q_k\)</span> and its corresponding document page <span class=arithmatex>\(d_k\)</span>.</li> <li><strong>Negative Pair</strong>: A query <span class=arithmatex>\(q_k\)</span> and a non-relevant document page <span class=arithmatex>\(d_l\)</span> (where <span class=arithmatex>\(l \neq k\)</span>).</li> </ul> <p>For a given batch of <strong><span class=arithmatex>\(b\)</span></strong> query-document pairs, this loss encourages the model to maximize the interaction score for correct query-document pairs while minimizing it for incorrect ones.</p> <ol> <li><strong>Late Interaction Similarity</strong>: Using the <strong>Late Interaction</strong> mechanism we discussed earlier, the similarity between the query <span class=arithmatex>\(q_k\)</span> and its relevant document <span class=arithmatex>\(d_k\)</span> is computed as:</li> </ol> <div class=arithmatex>\[ s^+_k = LI(q_k, d_k) \]</div> <ol> <li><strong>Negative Similarity</strong>: Similarly, the similarity between the query <span class=arithmatex>\(q_k\)</span> and any other document page <span class=arithmatex>\(d_l\)</span> is computed using the <strong>max similarity</strong> from Late Interaction: </li> </ol> <div class=arithmatex>\[ s^-_k = \max_{l \neq k} LI(q_k, d_l) \]</div> <ol> <li><strong>In-Batch Contrastive Loss</strong>: The final <strong>contrastive loss</strong> for a batch of size <strong><span class=arithmatex>\(b\)</span></strong> is calculated using <strong>softmaxed cross-entropy</strong> between the positive score and all the negative scores:</li> </ol> <div class=arithmatex>\[ Loss (L) = - \frac{1}{b} \sum_{k=1}^{b} \log \frac{\exp(s^+_k)}{\sum_{l=1}^{b} \exp(s^-_l)} \]</div> <hr> <h4 id=but-why-these-1b-log-exp-etc-etc-if-thats-not-making-sense-to-you-then-lets-understand-the-intuition-behind-the-formula><a class=toclink href=../../colpali-indepth-blog/#but-why-these-1b-log-exp-etc-etc-if-thats-not-making-sense-to-you-then-lets-understand-the-intuition-behind-the-formula>💡 But why these -1/b, log, exp, etc. etc.? If that’s not making sense to you, then let's understand the intuition behind the formula.</a></h4> <ul> <li> <p><strong>Breaking down the contrastive loss formula and reasoning behind it in detail.</strong></p> <h4 id=negative-sign-and-averaging-1b><a class=toclink href=../../colpali-indepth-blog/#negative-sign-and-averaging-1b>Negative Sign and Averaging (-1/b)</a></h4> <p>The loss starts with a <strong>negative sign</strong> and is multiplied by <span class=arithmatex>\(\frac{-1}{b}\)</span>, where <span class=arithmatex>\(b\)</span> is the batch size. Let’s explain the intuition behind this:</p> <ul> <li><strong>Why Negative?</strong> The negative sign is present because we are using the <strong>logarithm</strong> of the probability of the positive pair being the correct match (more on why logarithm later). Since probabilities are less than or equal to 1, their log is always <strong>negative</strong> (logarithm of a value between 0 and 1 is negative). The negative sign ensures that the loss value is <strong>positive</strong>. The negative log likelihood is a standard way to compute the loss when working with probabilities.</li> <li><strong>Why</strong> <span class=arithmatex>\(\frac{-1}{b}\)</span><strong>?</strong> It’s a standard <strong>averaging</strong> technique used to normalize the loss over the batch size, to ensure that the training process is <strong>stable</strong>, regardless of the batch size.</li> </ul> <h4 id=logarithm-log><a class=toclink href=../../colpali-indepth-blog/#logarithm-log>Logarithm (log)</a></h4> <p>The log in the contrastive loss plays a crucial role in how the model is trained.</p> <ul> <li> <p><strong>Why Log?</strong> The logarithm is used because we are dealing with <strong>probabilities</strong>. Specifically, the model outputs a similarity score for the positive pair, and we want to convert this score into a <strong>probability</strong>. The <strong>logarithm</strong> is applied to penalize incorrect predictions more strongly as the output gets farther from the correct value.</p> <p>When a model predicts a value close to 1 for the positive pair (high similarity), the log of this value will be small (i.e., minimal penalty). However, if the model predicts a low similarity for the correct pair (close to 0), the log value will be very <strong>large and negative</strong>, resulting in a larger penalty.</p> <p>This <strong>logarithmic scaling</strong> makes the model more sensitive to large errors, driving the optimization to <strong>shrink</strong> the loss for confident predictions and penalize wrong predictions.</p> </li> </ul> <h4 id=exponentiation-exp-in-the-numerator-and-denominator><a class=toclink href=../../colpali-indepth-blog/#exponentiation-exp-in-the-numerator-and-denominator>Exponentiation (exp) in the Numerator and Denominator</a></h4> <ul> <li><strong>Why Use Exponentiation?</strong> The reason we use exponentiation is to convert the similarity score into a <strong>probability</strong> through the <strong>softmax function</strong>. The softmax function is commonly used to calculate probabilities in multi-class classification tasks.<ul> <li>The <strong>numerator</strong> <span class=arithmatex>\(\exp(s^+_k)\)</span> represents the <strong>exponentiated similarity</strong> between the query <span class=arithmatex>\(q_k\)</span> and its corresponding positive document <span class=arithmatex>\(d_k\)</span>. By exponentiating the similarity score, the softmax function ensures that higher similarities get larger probabilities, and lower similarities get smaller probabilities.</li> <li>The <strong>denominator</strong> <span class=arithmatex>\(\sum_{l=1}^{b} \exp(s^-_l)\)</span> sums up all the <strong>negative similarities</strong> for the other documents in the batch. This sum is used to <strong>normalize</strong> the probability of the positive pair, ensuring that the output is between 0 and 1.</li> </ul> </li> <li><strong>Why Apply <span class=arithmatex>\(\exp(s^+_k)\)</span> to the Positive Pair?</strong> The <strong>positive pair</strong> is placed in the numerator because we want the <strong>highest probability</strong> to be assigned to the correct query-document pair. By applying the exponential function, we are amplifying the differences in similarity scores, ensuring that <strong>higher similarity scores</strong> result in <strong>higher probabilities</strong>.</li> </ul> <h4 id=why-maximum-similarity-for-negative-pairs><a class=toclink href=../../colpali-indepth-blog/#why-maximum-similarity-for-negative-pairs>Why Maximum Similarity for Negative Pairs?</a></h4> <p>When computing the negative similarity, we use the <strong>maximum similarity</strong> between the query and all incorrect document pages <span class=arithmatex>\(d_l\)</span>, where <span class=arithmatex>\(l \neq k\)</span>. This is because in the contrastive loss, we want to ensure that the model learns to <strong>separate</strong> the positive pair from the <strong>most similar negative pair</strong>.</p> <p>The model may encounter many incorrect documents in a batch, but only the documents with most similar scores one poses a risk of confusion with the correct match. By focusing on the <strong>maximum similarity</strong> among the negative pairs, the model learns to reduce the chance of <strong>confusion</strong> between the correct match and the <strong>most similar incorrect documents</strong>.</p> <p>If we didn’t use the maximum and instead averaged all the negative similarities, the model might not learn to focus on the most difficult negatives (i.e., those that are similar to the correct document but still incorrect).</p> </li> </ul> <hr> <h4 id=2-dataset-and-data-augmentation><a class=toclink href=../../colpali-indepth-blog/#2-dataset-and-data-augmentation><strong>2. Dataset and Data Augmentation</strong></a></h4> <h4 id=training-dataset><a class=toclink href=../../colpali-indepth-blog/#training-dataset><strong>Training Dataset</strong>:</a></h4> <p>ColPali is trained on a massive dataset of <strong>127,460 query-document pairs</strong> composed of both <strong>real and synthetic data</strong>:</p> <ul> <li><strong>63% of the data</strong> comes from real, publicly available academic datasets.</li> <li><strong>37% of the data</strong> is synthetic, created by web-crawling PDF documents and generating queries using a <strong>Vision-Language Model (VLM)</strong> called <strong>Claude-3 Sonnet</strong>.</li> </ul> <p>This mix of real and synthetic data helps ColPali generalize well to unseen documents. Interestingly, the training dataset is <strong>fully English</strong>, but ColPali shows strong <strong>zero-shot generalization</strong> to non-English languages.</p> <h4 id=query-augmentation><a class=toclink href=../../colpali-indepth-blog/#query-augmentation><strong>Query Augmentation</strong>:</a></h4> <p>Inspired by <strong>ColBERT</strong>, ColPali uses a technique called <strong>Query Augmentation</strong>, where 5 <strong><unused> tokens</strong> are appended to the query during training. These tokens are placeholders, and while they don’t have any predefined meaning, they can act as <strong>learnable parameters</strong> that help the model adjust its attention during the retrieval process. These unused tokens allow the model to:</p> <ul> <li><strong>Expand</strong> the query dynamically.</li> <li><strong>Re-weight</strong> query tokens, making it easier for the model to learn which parts of the query are most important for retrieval.</li> </ul> <p><strong>Mathematical Intuition</strong>:</p> <ul> <li>Let the original query be <span class=arithmatex>\(q = \{q_1, q_2, \dots, q_n\}\)</span></li> <li>We append 5 <strong><unused> tokens</strong> to this query, so the final query becomes:</li> </ul> <p><span class=arithmatex>\(q' = \{q_1, q_2, \dots, q_n, \text{&lt;unused1&gt;}, \text{&lt;unused2&gt;}, \dots, \text{&lt;unused5&gt;}\}\)</span></p> <p>These unused tokens are treated like regular query tokens during training but allow the model to adjust the importance of certain parts of the query during retrieval.</p> <p>I won’t further explain query augmentation in detail, please refer ColBERT’s paper for more detailed explanation: <a href=https://arxiv.org/pdf/2004.12832>ColBERT paper</a></p> <hr> <h4 id=3-low-rank-adapters-lora-for-efficient-training><a class=toclink href=../../colpali-indepth-blog/#3-low-rank-adapters-lora-for-efficient-training><strong>3. Low-Rank Adapters (LoRA) for Efficient Training</strong></a></h4> <p>Training large models from scratch is computationally expensive and often impractical. To make the training process more efficient, ColPali uses <strong>Low-Rank Adapters (LoRA)</strong>, a technique that allows fine-tuning a small subset of the model’s parameters without requiring the full model to be updated.</p> <p><strong>How LoRA Works:</strong></p> <p>In the transformer layers of the language model:</p> <ul> <li><strong>LoRA</strong> adds a <strong>low-rank matrix</strong> to the attention weights and <strong>only fine-tunes</strong> these low-rank matrices during training.</li> <li>This drastically reduces the number of trainable parameters, making fine-tuning more efficient.</li> </ul> <p>Again, not going in-depth into LoRA here (it’s out of scope of this blog)</p> <hr> <h3 id=bisiglip-and-bipali-embedding-techniques><a class=toclink href=../../colpali-indepth-blog/#bisiglip-and-bipali-embedding-techniques><strong>BiSigLIP and BiPali: Embedding Techniques</strong></a></h3> <h4 id=bisiglip-visual-textual-embedding-model><a class=toclink href=../../colpali-indepth-blog/#bisiglip-visual-textual-embedding-model><strong>BiSigLIP: Visual-Textual Embedding Model</strong></a></h4> <p>ColPali builds on <strong>SigLIP</strong>, a vision-language bi-encoder model. SigLIP is pre-trained on <strong>WebLI</strong>, a massive corpus of billions of <strong>image-text pairs</strong>. In ColPali, SigLIP is fine-tuned on a <strong>document retrieval dataset</strong>, which allows it to handle the <strong>visual and textual elements</strong> of documents more effectively.</p> <ol> <li><strong>SigLIP as a Bi-Encoder</strong>: SigLIP generates embeddings for both <strong>image patches</strong> and <strong>text</strong> and aligns them in the same latent space, enabling cross-modal comparison between <strong>text queries</strong> and <strong>document images</strong>.</li> <li><strong>BiSigLIP Fine-Tuning</strong>: ColPali fine-tunes SigLIP on a <strong>document-oriented dataset</strong> to improve its retrieval performance.</li> </ol> <hr> <h4 id=bipali-pairing-image-patches-with-paligemma><a class=toclink href=../../colpali-indepth-blog/#bipali-pairing-image-patches-with-paligemma><strong>BiPali: Pairing Image Patches with PaliGemma</strong></a></h4> <p>In the <strong>BiPali</strong> model, <strong>SigLIP-generated image patch embeddings</strong> are passed through a <strong>language model (PaliGemma)</strong> to obtain <strong>contextualized output embeddings</strong>. This enhances the embeddings by adding <strong>language-based context</strong>, which helps in tasks that require understanding the relationship between text and images.</p> <ul> <li><strong>Pooling Operation</strong>: The image patch embeddings are <strong>average pooled</strong> to create a single dense vector representing the entire document. Remember, <strong>BiPali</strong> initially explored <strong>pooling</strong> as a way to create a single dense vector from image patch embeddings. But, <strong>ColPali</strong> does <strong>not use pooling</strong>.</li> </ul> <hr> <h4 id=late-interaction><a class=toclink href=../../colpali-indepth-blog/#late-interaction><strong>Late Interaction</strong></a></h4> <p>The <strong>ColPali</strong> model further extends this by introducing token-level interaction between text and image embeddings, which drastically improves performance on complex visual tasks.</p> <hr> <p>As we near the conclusion of this blog, wouldn't you be curious to explore the outcomes of Colpali's experiments with various architectures and techniques? Let's dive in!</p> <h3 id=results-and-lessons-from-colpalis-iterative-development><a class=toclink href=../../colpali-indepth-blog/#results-and-lessons-from-colpalis-iterative-development><strong>Results and Lessons from ColPali’s Iterative Development</strong></a></h3> <p>In constructing <strong>ColPali</strong>, the authors iteratively built and improved upon various models, starting with an off-the-shelf <strong>SigLIP</strong> model, followed by pairing SigLIP with a language model (<strong>PaliGemma</strong>) and finally adding <strong>Late Interaction</strong> (as seen above). Each iteration provided insights into the model's performance across different document retrieval tasks, particularly for documents with complex visual elements like tables and figures.</p> <h4 id=improvements-with-bisiglip><a class=toclink href=../../colpali-indepth-blog/#improvements-with-bisiglip><strong>Improvements with BiSigLIP</strong></a></h4> <p>When fine-tuned on the <strong>document retrieval dataset</strong>, <strong>BiSigLIP</strong> showed significant improvements across various document retrieval tasks:</p> <ul> <li><strong>ArxivQA (figure retrieval)</strong>: Focused on retrieving figures from academic papers.</li> <li><strong>TabFQuAD (table retrieval)</strong>: Tasked with retrieving tables from documents.</li> </ul> <p>By further fine-tuning SigLIP’s text and image encoders on this <strong>document-specific dataset</strong>, ColPali achieved improved performance for tasks requiring understanding both <strong>textual and visual information</strong>.</p> <hr> <h4 id=performance-of-bipali><a class=toclink href=../../colpali-indepth-blog/#performance-of-bipali><strong>Performance of BiPali</strong></a></h4> <p>After fine-tuning on the training dataset, <strong>BiPali</strong> showed a slight decrease in performance for <strong>English document retrieval tasks</strong> compared to <strong>BiSigLIP</strong>. This is likely due to the fact that <strong>PaliGemma</strong> was not originally trained for <strong>contrastive matching tasks</strong>, but rather for <strong>next token prediction</strong>. The authors' <strong>contrastive fine-tuning</strong> on 100K images was significantly smaller than SigLIP’s original contrastive training, leading to <strong>weaker performance in English retrieval tasks</strong>.</p> <ul> <li> <p><strong>Why BiPali Shines in Multilingual Tasks</strong></p> <p>Despite the slight drop in English performance, <strong>BiPali</strong> showed <strong>notable improvements in French tasks</strong>, indicating that <strong>PaliGemma’s multilingual pretraining</strong> helps with <strong>multilingual document retrieval</strong>. Interestingly, although the training dataset did not contain any non-English samples, <strong>PaliGemma’s LLM</strong> (Gemma-2B) was able to handle multilingual data, resulting in better cross-lingual generalization.</p> </li> </ul> <hr> <h4 id=why-late-interaction-improves-performance><a class=toclink href=../../colpali-indepth-blog/#why-late-interaction-improves-performance><strong>Why Late Interaction Improves Performance</strong></a></h4> <p>By focusing on the <strong>most relevant document patches</strong> for each query token, <strong>Late Interaction</strong> enables ColPali to excel in tasks that require <strong>detailed understanding of both text and visual elements</strong>. This was especially evident in more <strong>visually complex tasks</strong>, such as:</p> <ul> <li><strong>InfographicVQA</strong> (infographic retrieval).</li> <li><strong>ArxivQA</strong> (figure retrieval).</li> <li><strong>TabFQuAD</strong> (table retrieval).</li> </ul> <p>ColPali outperformed baselines such as <strong>Unstructured and captioning-based models</strong>, as well as all evaluated <strong>text-image embedding models</strong>. This stark improvement in visually complex tasks demonstrates the power of <strong>Late Interaction</strong> in multimodal retrieval.</p> <hr> <h4 id=lessons-from-negative-results-colsiglip-and-bisiglip-with-late-interaction><a class=toclink href=../../colpali-indepth-blog/#lessons-from-negative-results-colsiglip-and-bisiglip-with-late-interaction><strong>Lessons from Negative Results: ColSigLIP and BiSigLIP with Late Interaction</strong></a></h4> <h4 id=colsiglip><a class=toclink href=../../colpali-indepth-blog/#colsiglip><strong>ColSigLIP</strong>:</a></h4> <p>A version of <strong>BiSigLIP</strong> with <strong>Late Interaction</strong> (ColSigLIP) was also tested but <strong>performed poorly</strong>. The authors attribute this to the fact that in <strong>SigLIP’s pre-training</strong>, only a <strong>pooled latent representation</strong> is used in the contrastive loss, which <strong>does not optimize individual patch and token embeddings</strong> as effectively as ColPali.</p> <h4 id=bisiglip-paligemma><a class=toclink href=../../colpali-indepth-blog/#bisiglip-paligemma><strong>BiSigLIP + PaliGemma</strong>:</a></h4> <p>The authors also experimented with using the <strong>text representations from PaliGemma</strong> and the <strong>image representations from SigLIP</strong>. However, this variant also performed poorly, likely due to a <strong>misalignment</strong> between the <strong>SigLIP embeddings</strong> and the <strong>Gemma embeddings</strong> after PaliGemma fine-tuning.</p> <hr> <h4 id=querying-latencies-and-memory-footprint><a class=toclink href=../../colpali-indepth-blog/#querying-latencies-and-memory-footprint><strong>Querying Latencies and Memory Footprint</strong></a></h4> <p>One of the challenges with <strong>Late Interaction</strong> is that it can increase <strong>querying latency</strong>. In online querying scenarios:</p> <ul> <li><strong>BGE-M3 embedding model</strong> takes approximately <strong>22 ms</strong> to encode a query with 15 tokens.</li> <li>In contrast, <strong>ColPali</strong> takes about <strong>30 ms</strong> for query encoding due to the <strong>additional complexity</strong> introduced by the language model.</li> </ul> <p>However, for smaller document corpora, the <strong>overhead from Late Interaction</strong> is minimal—approximately <strong>1 ms per 1000 pages</strong>. This makes ColPali a <strong>scalable solution</strong> for document retrieval, even in larger datasets.</p> <h4 id=conclusion-building-colpalifrom-siglip-to-late-interaction><a class=toclink href=../../colpali-indepth-blog/#conclusion-building-colpalifrom-siglip-to-late-interaction><strong>Conclusion: Building ColPali—From SigLIP to Late Interaction</strong></a></h4> <p>The iterative construction of <strong>ColPali</strong> highlights how combining <strong>vision-language models</strong> (SigLIP), <strong>language models</strong> (PaliGemma), and <strong>Late Interaction</strong> led to a state-of-the-art document retrieval system. Each step added new capabilities:</p> <ul> <li><strong>SigLIP</strong> provided strong bi-modal embeddings.</li> <li><strong>PaliGemma</strong> enhanced these embeddings with <strong>language model context</strong>.</li> <li><strong>Late Interaction</strong> enabled ColPali to excel in <strong>token-level retrieval</strong>, focusing on the most relevant parts of documents for each query.</li> </ul> <hr> <p>pheww! it was a long long blog! I really hope this blog can help in understanding about colpali in detail. cheers to all the readers for sticking till the end!</p> </div> </article> <nav class=md-pagination> </nav> </div> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../colpali/ class="md-footer__link md-footer__link--prev" aria-label="Previous: ColPali"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> ColPali </div> </div> </a> <a href=../quick-notes/ class="md-footer__link md-footer__link--next" aria-label="Next: Quick Notes"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Quick Notes </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024 Scaled Focu </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://twitter.com/nirantk target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </a> <a href=https://github.com/nirantk target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://www.linkedin.com/in/nirant/ target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.83f73b43.min.js></script> <script src=../../../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>