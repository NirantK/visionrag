{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/","title":"Blog","text":"<p>Hello, we write about VisionRAG. </p> <p>We won't email you more than twice a month, not every post I write is worth sharing but we'll do our best to share the most interesting stuff including our own writing, thoughts, and experiences.</p>"},{"location":"blog/beyond-basic-rag/","title":"Beyond Basic RAG: What You Need to Know","text":"<p>The Real World of RAG Systems</p> <p>\ud83d\udcd2 Picture this: You're a developer who just deployed your first RAG system. Everything seems perfect in testing. Then reality hits - users start complaining about irrelevant results, not being able to do \"basic stuff\" and occasional hallucinations. Welcome to the world of real-world RAG systems.</p>"},{"location":"blog/beyond-basic-rag/#the-problem-with-naive-rag","title":"The Problem With \"Naive RAG\"","text":"<p>Let's start with a truth bomb: dumping documents into a vector database and hoping for the best is like trying to build a search engine with just a dictionary - technically possible, but practically useless.</p> <p>Here's why:</p> <ol> <li> <p>The Embedding Trap: Think embedding similarity is enough? Here's a fun fact - in many embedding models, \"yes\" and \"no\" have a similarity of 0.8-0.9. Imagine asking for \"yes\" and getting a \"no\" instead in a legal search \ud83d\ude05</p> </li> <li> <p>The Context Confusion: Large Language Models (LLMs) get surprisingly confused when you give them unrelated information. They're like that friend who can't ignore a app notification while telling a story - everything gets mixed up. </p> </li> <li> <p>Length Effect: Just like humans tend to get worse at noticing details the longer a story is, LLMs with large context windows get worse at noticing details the longer the information is.</p> </li> </ol>"},{"location":"blog/beyond-basic-rag/#the-three-pillars-of-production-rag","title":"The Three Pillars of Production RAG","text":""},{"location":"blog/beyond-basic-rag/#1-query-understanding","title":"1. Query Understanding \ud83c\udfaf","text":"<p>The first step to better RAG isn't about better embeddings - it's about understanding what your users are actually asking for. Here's the basics:</p> <ul> <li>Query Classification: Before rushing to retrieve documents, classify the query type. Is it a simple lookup? A comparison? An aggregation? Each needs different handling.<ul> <li>NIT: Navigational, Informational, Transactional are the 3 very broad types.</li> </ul> </li> <li>Metadata Extraction: Time ranges, entities, filters - extract these before retrieval. Think of it as giving your students sample questions to help them pay attention to what's important in the exam (at query time) much better and faster. </li> </ul> <p>Metadata Queries</p> <p>The CEO of a company asks for \"last year's revenue\"</p> <p>The CFO asks for \"revenue from last year\"</p> <p>The CMO asks for \"revenue from the last fiscal year\"</p> <p>Do all these queries mean different things? Not really. The asker role i.e. query metadata changes the query intent.</p>"},{"location":"blog/beyond-basic-rag/#2-intelligent-retrieval-strategies","title":"2. Intelligent Retrieval Strategies \ud83d\udd0d","text":"<p>Here's where most systems fall short. Instead of one-size-fits-all retrieval:</p> <ul> <li>Hybrid Search: Combine dense (embedding) and sparse (keyword) retrieval. You can rerank using late interaction, use LLM as a reranker or even use both in a cascade. I can probably write a whole blog post on this, but tl;dr is that you can use a combination of many retrieval strategies to get the best of precision, recall, cost and latency.</li> <li>Query Expansion: Don't just search for what users ask - search for what they mean. Example: \"Q4 results\" should also look for \"fourth quarter performance.\"</li> <li>Context-Aware Filtering: Use metadata to filter before semantic search. If someone asks for \"last week's reports,\" don't rely on embeddings to figure out the time range.</li> </ul>"},{"location":"blog/beyond-basic-rag/#3-result-synthesis-and-validation","title":"3. Result Synthesis and Validation \u2705","text":"<p>The final piece is making sure your responses are accurate and useful:</p> <ul> <li>Cross-Validation: For critical information (dates, numbers, facts), validate across multiple sources at ingestion time. It's possible that your ingestion pipeline is flawed and you don't know it.</li> <li>Readability Checks: Use tools like the Flesch-Kincaid score to ensure responses match your user's expertise level.</li> <li>Hallucination Detection: Implement systematic checks for information that isn't grounded in your retrieved documents. Considering evaluating the pipeline using offline tools like Ragas</li> </ul>"},{"location":"blog/beyond-basic-rag/#real-world-example-the-leave-policy-fiasco","title":"Real-World Example: The Leave Policy Fiasco","text":"<p>Here's a real story that illustrates why naive RAG fails:</p> <p>The Leave Policy Fiasco</p> <p>Company X implemented a RAG system for HR queries. When employees asked about leave policies,  the system kept used the entire company's wiki -- including that of the sales team.  And sales \"ranked\" higher because it contained similar keywords. </p> <p>The result? The entire company was getting sales team vacation policies instead of their own \ud83e\udd26\u200d\u2642\ufe0f</p> <p>The solution? They implemented:</p> <ol> <li> <p>Role-based filtering</p> </li> <li> <p>Document source validation</p> </li> <li> <p>Query intent classification</p> </li> </ol>"},{"location":"blog/beyond-basic-rag/#making-your-rag-system-production-ready","title":"Making Your RAG System Production-Ready","text":"<p>Here's your action plan:</p> <ol> <li>Query Understanding: Implement basic query type classification</li> <li>Ingestion: Extract key metadata (dates, entities, filters)</li> <li>Retrieval: Begin with metadata filtering</li> <li>Retrieval: Add keyword-based search or BM25</li> <li>Retrieval: Top it off with semantic search</li> <li>Synthesis: Combine results intelligently using a good re-ranker or fusion e.g. RRF</li> <li>Validation: Cross-check extracted dates and numbers</li> <li>Validation: Implement a RAG metrics system e.g. Ragas</li> <li>Validation: Monitor user feedback e.g. using A/B tests and adapt </li> </ol> <p>Reciprocal Rank Fusion</p> <p>Reciprocal Rank Fusion (RRF) is a technique that combines the results of multiple retrieval systems. It's a powerful way to improve the quality of your search results by leveraging the strengths of different retrieval methods. </p> <p>But it's NOT a silver bullet.</p>"},{"location":"blog/beyond-basic-rag/#the-challenge","title":"The Challenge","text":"<p>Stop thinking about RAG as just \"retrieve and generate.\" </p> <p>Start thinking about it as a complex system that needs to understand, retrieve, validate, and synthesize information intelligently.</p> <p>Your homework: Take one query type that's failing in your system. Implement query classification and targeted retrieval for just that type. Measure the improvement. You'll be amazed at the difference this focused approach makes.</p> <p>Remember: The goal isn't to build a perfect RAG system (that doesn't exist). The goal is to build a RAG system that improves continuously and fails gracefully.</p> <p>Your Turn</p> <p>What's your biggest RAG challenge? Let's solve it together. Let me know on Twitter or email.</p>"},{"location":"blog/colpali-indepth-blog/","title":"ColPali: Document Retrieval with Vision Language Models \u2728","text":"<p>\ud83c\udff9 We will be diving deep into the paper: [Arxiv] ColPali: Efficient Document Retrieval with Vision Language Models</p> <p>Document retrieval has always been a key component of systems like search engines and information retrieval. Traditional document retrieval methods rely heavily on text-based methods (like OCR and text segmentation), often missing crucial visual cues like layouts, images, and tables.</p> <p>ColPali addresses this by using Vision-Language Models (VLMs) to understand and retrieve visually rich documents, capturing both textual and visual information. ColPali's architecture allows direct encoding of document images into a common embedding space, eliminating the need for time-consuming text extraction and segmentation.</p> <p>In this blog, we\u2019ll explore the technicalities behind ColPali covering the topics:</p> <ul> <li>Architecture of ColPali</li> <li>Training techniques <ul> <li>Contrastive Loss for query-document matching.</li> <li>Training on 127,460 query-document pairs (real + synthetic data).</li> </ul> </li> <li>How it transforms embeddings using techniques like: <ul> <li>BiSigLIP for visual-textual embeddings.</li> <li>BiPali for pairing image patches with PaliGemma.</li> <li>Late Interaction to achieve state-of-the-art performance.</li> </ul> </li> </ul> <p></p> <p>Before we dive into the technical architecture and training of ColPali, let\u2019s walk through the intuition behind how it works.</p>"},{"location":"blog/colpali-indepth-blog/#intuition-behind-colpali-how-it-simplifies-document-retrieval","title":"Intuition Behind ColPali: How It Simplifies Document Retrieval","text":""},{"location":"blog/colpali-indepth-blog/#step-1-treating-the-document-as-an-image","title":"Step 1: Treating the Document as an Image","text":"<p>Imagine we have a PDF document. Normally, we would extract text from the document using OCR (Optical Character Recognition), segment it into different sections, and then use these segments for searching. ColPali simplifies this process by treating the entire document page as an image, bypassing the need for complex text extraction.</p> <ul> <li>Think of it as taking a photograph of each page of the document. No need to convert the text, handle different languages, or worry about complex layouts. Just treat the page like a picture.</li> </ul>"},{"location":"blog/colpali-indepth-blog/#step-2-splitting-the-image-into-patches","title":"Step 2: Splitting the Image into Patches","text":"<p>Once ColPali has this \"image\" of the document, it divides the page into small, uniform pieces called patches (for eg, 16x16 pixels).</p> <ul> <li>Each patch captures a tiny portion of the page\u2014it might contain a few words, a piece of a graph, or part of an image. This division helps the model to focus on small, detailed parts of the document rather than trying to understand the whole page at once.</li> <li> <p>How are patches diff from chunking? Will it not lose context in between?</p> <p>At first glance, it might seem like dividing an image into patches is similar to breaking text into chunks. However, there are several key differences between these two methods, especially in how they handle and preserve context. Let\u2019s dive deeper into these differences to understand why patch-based processing in ColPali is more effective for document retrieval compared to traditional text chunking.</p> </li> </ul>"},{"location":"blog/colpali-indepth-blog/#understanding-context-loss-in-text-chunking","title":"Understanding Context Loss in Text Chunking","text":"<p>In traditional text chunking, text is split into smaller chunks based on a certain number of tokens since many models have a limit on the number of tokens they can process at once.</p> <p>Problem with Context Loss:</p> <ul> <li>Chunking can split sentences or paragraphs mid-way, losing crucial contex and can result in incomplete information in one chunk and missing context in another.</li> <li>Chunking doesn't preserve visual or structural information, like the relationship between headings and their corresponding content or the placement of text in tables or figures.</li> </ul> <p>Visual Example: If you have a document with a heading followed by a table, text chunking might separate the heading and the table, losing the context that the table belongs to that heading.</p>"},{"location":"blog/colpali-indepth-blog/#patch-based-image-processing-in-colpali","title":"Patch-Based Image Processing in ColPali","text":"<p>ColPali divides the document image into patches, much like dividing a photo into small squares. Each patch is a fixed-size portion of the image, like a mini-snapshot of that part of the page.</p> <p>Why Patches are More Effective:</p> <p>No Loss of Structure: The patches retain the visual structure of the document, preserving its spatial layout. For instance, if a page has two columns of text or a table with rows and columns, each patch maintains its relative position, ensuring that the model understands the overall arrangement of the elements.</p> <p>Multi-Modal Context: Patches capture both textual and visual information. This includes both visual features (e.g., font styles, colors, boldness) and non-text elements (e.g., figures and graphs).</p> <p>Positional Awareness: Each patch has a positional embedding that tells the model where it is located on the page, helping the model understand the overall layout.</p>"},{"location":"blog/colpali-indepth-blog/#step-3-embedding-creation-and-aligning-visual-and-textual-information","title":"Step 3: Embedding Creation and Aligning Visual and Textual Information","text":"<p>Each patch is then passed through a Vision Transformer (ViT), which converts them into unique embeddings\u2014mathematical codes that represent the content within each patch.</p> <p>Next, ColPali aligns these visual embeddings with the text of the query by transforming the query into its own set of embeddings. ColPali uses a process called alignment, that aligns image path embeddings and text embeddings in the same vector space only then we can compare the similarity between query and document embeddings. </p>"},{"location":"blog/colpali-indepth-blog/#step-4-scoring-the-relevance-late-interaction-mechanism","title":"Step 4: Scoring the Relevance - Late Interaction Mechanism","text":"<p>At this point, ColPali has embeddings for both the query and the document. The next challenge is to identify the relevant parts of the document. ColPali uses a process called the Late Interaction Mechanism, where each piece of the query is finely matched against every part of the document, scoring and ranking their relevance.</p> <p>ColPali highlights the most relevant pieces of the document, focusing on the patches that best match the query. This approach enables ColPali to efficiently retrieve relevant information from visually rich documents, capturing both visual and textual data without losing context.</p> <p>Let\u2019s dive deeper now\u2026</p>"},{"location":"blog/colpali-indepth-blog/#colpali-architecture-a-detailed-exploration","title":"ColPali Architecture: A Detailed Exploration","text":"<p>ColPali\u2019s architecture is an innovative document retrieval system that handles visual data exclusively. ColPali directly processes document images, representing them as image patches. When a query (in textual form) is introduced, it is embedded into the same shared latent space as the image patches, allowing seamless comparison between the two.</p>"},{"location":"blog/colpali-indepth-blog/#1-vision-language-models-vlms-aligning-text-and-image-embeddings-in-shared-space","title":"1. Vision-Language Models (VLMs): Aligning Text and Image Embeddings in Shared Space","text":"<p>ColPali leverages Vision-Language Models (VLMs) to generate embeddings for both the visual (image patches) and textual content of documents. These image patches are treated as tokens, much like how words are treated in language models. The specific model extended by ColPali is PaliGemma-3B, which is known for its efficiency in processing visually rich documents.</p> <p>PaliGemma-3B combines Vision Transformers (ViTs) for visual data with language models for textual data, creating a shared representation space for both. The vision transformer breaks down an input image into smaller patches (e.g., 16x16 pixels), and each patch is treated as a separate token. These tokens are then passed through the transformer, similar to how text tokens are processed.</p> <p>Mathematical Intuition of Image Patch Embedding:</p> <ul> <li>Let \\(x_{\\text{img}} = \\{ p_1, p_2, \\dots, p_m \\}\\) represent the sequence of image patches from a document, where each patch \\(p_i\\) is a segment of the overall document image.</li> </ul> <p>The Vision Transformer (ViT) processes each patch independently and generates embeddings:</p> \\[ E_{\\text{img}} = \\text{VisionEncoder}(x_{\\text{img}}) = \\{ e_1, e_2, \\dots, e_m \\} \\] <p>where \\(E_{\\text{img}} \\in \\mathbb{R}^{m \\times D}\\) and \\(D\\) is the dimensionality of the embedding space.</p> <p>These embeddings are then compared to the embeddings of the text query, all within the same shared space.</p> <p>Text Query Embedding: When a query is provided, ColPali converts it into a sequence of tokens and passes these tokens through a language model. The same VLM architecture is employed to embed the text query into the same latent space as the image patches. This shared space ensures that the query embeddings can be directly compared with the document image embeddings.</p> <p>Mathematical Intuition:</p> <ul> <li>Let \\(x_{\\text{query}} = \\{ q_1, q_2, \\dots, q_n \\}\\) represent the query tokens, where n is the number of tokens in the query.</li> </ul> <p>The Text Encoder then embeds the query:</p> \\[ E_{\\text{query}} = \\text{TextEncoder}(x_{\\text{query}}) = \\{ e'_1, e'_2, \\dots, e'_n \\} \\] <p>where \\(E_{\\text{query}} \\in \\mathbb{R}^{n \\times D}\\)</p> <p>This means that both the image patches and the text query are represented in the same \\(D\\)-dimensional space, facilitating a direct comparison.</p> <p>Let\u2019s understand in detail the intuition behind alignment of text and image embeddings..</p>"},{"location":"blog/colpali-indepth-blog/#aligning-text-and-image-embeddings-in-vlm","title":"Aligning Text and Image Embeddings in VLM","text":"<p>One of the most fascinating challenges in vision-language models is that it must handle two very different types of data: text and images. These two modalities not only come from different domains but also have different dimensionalities. For VLM to work effectively, it needs to align these different representations so that the text and image data can be compared in a common space.</p>"},{"location":"blog/colpali-indepth-blog/#the-problem-different-sizes-different-worlds","title":"The Problem: Different Sizes, Different Worlds","text":"<p>VLM model has the ability to compare text queries (like \"Show me the sales report\") with visual elements from a document, such as a table or chart. But the embeddings that represent text and image data needs to exist in the same dimensional space. Say,</p> <ul> <li>Text Embeddings: Generated from transformer-based models (such as BERT, GPT), text embeddings typically have sizes like 768 dimensions or 1024 dimensions.</li> <li>Image Embeddings: Produced from Vision Transformers (ViT), image embeddings often have larger dimensions like 1024 or 2048 dimensions. These embeddings encode the visual features of image patches.</li> </ul> <p>Now the question now is, how do you compare text embeddings of size 768 with image embeddings of size 1024? We can\u2019t, since they are incompatible. </p>"},{"location":"blog/colpali-indepth-blog/#the-solution-linear-projections","title":"The Solution: Linear Projections","text":"<p>Linear projection is a technique used to bring vectors from different spaces into a common latent space. Here\u2019s the mathematical intuition behind it:</p> <p>Mapping Text Embeddings and Image Embeddings to the Shared Space</p> <p>Let\u2019s say our text embedding is of size 768 and image embedding of size 1024. We want to transform this into a new space of size 128 (this shared space dimension was used in colpali) so that it matches the dimensionality of the image embeddings.</p> <p>This is done using a linear projection:</p> \\[ E^{\\text{text}} = W_{text} \\cdot E_{text} + b_{text} \\] \\[ E^{\\text{image}} = W_{image} \\cdot E_{image} + b_{image} \\] <p>Where:</p> <ul> <li>\\(E_{text}\\) is your original text embedding (a vector of size 768).</li> <li>\\(W_{text} \\in \\mathbb{R}^{128 \\times 768}\\) is the weight matrix that transforms the text embedding from dimension 768 to 128.</li> <li>\\(b_{text} \\in \\mathbb{R}^{128}\\) is a bias term that shifts the transformed vector appropriately.</li> <li>\\(E_{image}\\) is the original image embedding (a vector of size 1024).</li> <li>\\(W_{image} \\in \\mathbb{R}^{128 \\times 1024}\\) is the weight matrix that transforms the image embeddings from dimension 1024 to 128.</li> <li>\\(b_{image} \\in \\mathbb{R}^{128}\\) is a bias term.</li> </ul> <p>This shared space enables cross-modal comparisons, meaning a textual query can retrieve relevant document images based on visual content, such as tables, charts, or other visual elements.</p> <p>\ud83c\udff9 Bonus: You might ask why 128 shared space dimension was used in Colpali? Any specific reason?</p> <p>To maintain a balance between memory efficiency and computational cost, while minimizing performance drops, we chose a 128-dimensional embedding. This dimension will later be used in a multi-vector reranking process, so smaller is better, but it\u2019s essential to strike the right balance for performance. Future versions are planned to offer the option to increase the embedding dimensions.</p>"},{"location":"blog/colpali-indepth-blog/#2-late-interaction-mechanism-for-finding-similarity-between-image-and-query","title":"2. Late Interaction Mechanism (for finding similarity between image and query)","text":"<p>The Late Interaction mechanism in ColPali is designed to perform token-level similarity matching between the text query and document image patches. Unlike traditional retrieval models that reduce everything into a single embedding vector for comparison, Late Interaction operates on individual token embeddings, preserving granular details and improving accuracy in retrieval tasks.</p> <p>The key idea is that for each query token, ColPali finds the most relevant image patch in the document. The model then aggregates these relevance scores to compute the overall similarity between the query and the document.</p> <p>Similarity Scoring with Late Interaction</p> <p>Late Interaction computes the similarity between each query token and each image patch. For each query token, it identifies the most relevant image patch using MaxSim (maximum similarity). Finally, the total similarity score between the query and the document is the sum of the maximum similarities for all query tokens.</p>"},{"location":"blog/colpali-indepth-blog/#mathematical-intuition-behind-late-interaction","title":"Mathematical Intuition Behind Late Interaction:","text":"<ul> <li>Dot product: For each query token \\(e_i\u2032\\) compute its similarity with each image patch \\(e_j\\) using the dot product. This gives a measure of how well query token \\(e_i\u2032\\) aligns with image patch \\(e_j\\).</li> </ul> \\[ \\text{Similarity}(e'_i, e_j) = e_i' \\cdot e_j \\] <ul> <li>MaxSim (Maximum Similarity): For each query token \\(e_i\u2032\\) , find the most similar image patch using the maximum similarity across all image patches. This ensures that only the most relevant image patch is considered for each query token.</li> </ul> \\[ \\text{MaxSim}(e'_i, E_{\\text{img}}) = \\max_{j=1}^{m} \\langle e'_i, e_j \\rangle \\] <ul> <li>Late Interaction Score: Finally, the total similarity score between the query and the document is computed by summing up the maximum similarities for all query tokens.</li> </ul> \\[ LI(E_{\\text{query}}, E_{\\text{img}}) = \\sum_{i=1}^{n} \\text{MaxSim}(e'_i, E_{\\text{img}}) \\] <p>The term Late Interaction refers to the idea that instead of merging or averaging embeddings from both the query and the document before comparison (as is often done in traditional dense retrieval methods), we retain token-level interactions until later in the retrieval process.</p>"},{"location":"blog/colpali-indepth-blog/#training-techniques-and-process-in-colpali","title":"Training Techniques and Process in ColPali","text":"<p>Now we will deep-dive into how ColPali is trained to achieve its powerful document retrieval capabilities. The training process focuses on contrastive learning, multi-modal alignment (already covered), and the use of techniques like Low-Rank Adapters (LoRA). The dataset is built from both real and synthetic data, further enhancing ColPali\u2019s generalization capabilities. </p>"},{"location":"blog/colpali-indepth-blog/#1-contrastive-loss-for-query-document-matching","title":"1. Contrastive Loss for Query-Document Matching","text":"<p>Here, contrastive loss is used to train the model to correctly associate a query with its corresponding document page while distinguishing it from other irrelevant pages. The core idea behind constrastive loss is to pull together positive query-document pairs and push apart negative pairs in the embedding space, enabling the model to learn more discriminative representations for both the query and the document.</p>"},{"location":"blog/colpali-indepth-blog/#mathematical-intuition-of-contrastive-loss","title":"Mathematical Intuition of Contrastive Loss","text":"<p>Let\u2019s define:</p> <ul> <li>Positive Pair: A query \\(q_k\\) and its corresponding document page \\(d_k\\).</li> <li>Negative Pair: A query \\(q_k\\) and a non-relevant document page \\(d_l\\)  (where \\(l \\neq k\\)).</li> </ul> <p>For a given batch of \\(b\\) query-document pairs, this loss encourages the model to maximize the interaction score for correct query-document pairs while minimizing it for incorrect ones.</p> <ol> <li>Late Interaction Similarity: Using the Late Interaction mechanism we discussed earlier, the similarity between the query \\(q_k\\) and its relevant document \\(d_k\\) is computed as:</li> </ol> \\[ s^+_k = LI(q_k, d_k) \\] <ol> <li>Negative Similarity: Similarly, the similarity between the query \\(q_k\\) and any other document page \\(d_l\\) is computed using the max similarity from Late Interaction: </li> </ol> \\[ s^-_k = \\max_{l \\neq k} LI(q_k, d_l) \\] <ol> <li>In-Batch Contrastive Loss: The final contrastive loss for a batch of size \\(b\\)  is calculated using softmaxed cross-entropy between the positive score and all the negative scores:</li> </ol> \\[ Loss (L) = - \\frac{1}{b} \\sum_{k=1}^{b} \\log \\frac{\\exp(s^+_k)}{\\sum_{l=1}^{b} \\exp(s^-_l)} \\]"},{"location":"blog/colpali-indepth-blog/#but-why-these-1b-log-exp-etc-etc-if-thats-not-making-sense-to-you-then-lets-understand-the-intuition-behind-the-formula","title":"\ud83d\udca1 But why these -1/b, log, exp, etc. etc.? If that\u2019s not making sense to you, then let's understand the intuition behind the formula.","text":"<ul> <li> <p>Breaking down the contrastive loss formula and reasoning behind it in detail.</p> </li> </ul>"},{"location":"blog/colpali-indepth-blog/#negative-sign-and-averaging-1b","title":"Negative Sign and Averaging (-1/b)","text":"<p>The loss starts with a negative sign and is multiplied by \\(\\frac{-1}{b}\\), where \\(b\\) is the batch size. Let\u2019s explain the intuition behind this:</p> <ul> <li>Why Negative? The negative sign is present because we are using the logarithm of the probability of the positive pair being the correct match (more on why logarithm later). Since probabilities are less than or equal to 1, their log is always negative (logarithm of a value between 0 and 1 is negative). The negative sign ensures that the loss value is positive. The negative log likelihood is a standard way to compute the loss when working with probabilities.</li> <li>Why \\(\\frac{-1}{b}\\)? It\u2019s a standard averaging technique used to normalize the loss over the batch size, to ensure that the training process is stable, regardless of the batch size.</li> </ul>"},{"location":"blog/colpali-indepth-blog/#logarithm-log","title":"Logarithm (log)","text":"<p>The log in the contrastive loss plays a crucial role in how the model is trained.</p> <ul> <li> <p>Why Log? The logarithm is used because we are dealing with probabilities. Specifically, the model outputs a similarity score for the positive pair, and we want to convert this score into a probability. The logarithm is applied to penalize incorrect predictions more strongly as the output gets farther from the correct value.</p> <p>When a model predicts a value close to 1 for the positive pair (high similarity), the log of this value will be small (i.e., minimal penalty). However, if the model predicts a low similarity for the correct pair (close to 0), the log value will be very large and negative, resulting in a larger penalty.</p> <p>This logarithmic scaling makes the model more sensitive to large errors, driving the optimization to shrink the loss for confident predictions and penalize wrong predictions.</p> </li> </ul>"},{"location":"blog/colpali-indepth-blog/#exponentiation-exp-in-the-numerator-and-denominator","title":"Exponentiation (exp) in the Numerator and Denominator","text":"<ul> <li>Why Use Exponentiation? The reason we use exponentiation is to convert the similarity score into a probability through the softmax function. The softmax function is commonly used to calculate probabilities in multi-class classification tasks.<ul> <li>The numerator \\(\\exp(s^+_k)\\) represents the exponentiated similarity between the query \\(q_k\\) and its corresponding positive document \\(d_k\\). By exponentiating the similarity score, the softmax function ensures that higher similarities get larger probabilities, and lower similarities get smaller probabilities.</li> <li>The denominator \\(\\sum_{l=1}^{b} \\exp(s^-_l)\\) sums up all the negative similarities for the other documents in the batch. This sum is used to normalize the probability of the positive pair, ensuring that the output is between 0 and 1.</li> </ul> </li> <li>Why Apply \\(\\exp(s^+_k)\\) to the Positive Pair? The positive pair is placed in the numerator because we want the highest probability to be assigned to the correct query-document pair. By applying the exponential function, we are amplifying the differences in similarity scores, ensuring that higher similarity scores result in higher probabilities.</li> </ul>"},{"location":"blog/colpali-indepth-blog/#why-maximum-similarity-for-negative-pairs","title":"Why Maximum Similarity for Negative Pairs?","text":"<p>When computing the negative similarity, we use the maximum similarity between the query and all incorrect document pages \\(d_l\\), where \\(l \\neq k\\). This is because in the contrastive loss, we want to ensure that the model learns to separate the positive pair from the most similar negative pair.</p> <p>The model may encounter many incorrect documents in a batch, but only the documents with most similar scores one poses a risk of confusion with the correct match. By focusing on the maximum similarity among the negative pairs, the model learns to reduce the chance of confusion between the correct match and the most similar incorrect documents.</p> <p>If we didn\u2019t use the maximum and instead averaged all the negative similarities, the model might not learn to focus on the most difficult negatives (i.e., those that are similar to the correct document but still incorrect).</p>"},{"location":"blog/colpali-indepth-blog/#2-dataset-and-data-augmentation","title":"2. Dataset and Data Augmentation","text":""},{"location":"blog/colpali-indepth-blog/#training-dataset","title":"Training Dataset:","text":"<p>ColPali is trained on a massive dataset of 127,460 query-document pairs composed of both real and synthetic data:</p> <ul> <li>63% of the data comes from real, publicly available academic datasets.</li> <li>37% of the data is synthetic, created by web-crawling PDF documents and generating queries using a Vision-Language Model (VLM) called Claude-3 Sonnet.</li> </ul> <p>This mix of real and synthetic data helps ColPali generalize well to unseen documents. Interestingly, the training dataset is fully English, but ColPali shows strong zero-shot generalization to non-English languages.</p>"},{"location":"blog/colpali-indepth-blog/#query-augmentation","title":"Query Augmentation:","text":"<p>Inspired by ColBERT, ColPali uses a technique called Query Augmentation, where 5  tokens are appended to the query during training. These tokens are placeholders, and while they don\u2019t have any predefined meaning, they can act as learnable parameters that help the model adjust its attention during the retrieval process. These unused tokens allow the model to: <ul> <li>Expand the query dynamically.</li> <li>Re-weight query tokens, making it easier for the model to learn which parts of the query are most important for retrieval.</li> </ul> <p>Mathematical Intuition:</p> <ul> <li>Let the original query be \\(q = \\{q_1, q_2, \\dots, q_n\\}\\)</li> <li>We append 5  tokens to this query, so the final query becomes: <p>\\(q' = \\{q_1, q_2, \\dots, q_n, \\text{&lt;unused1&gt;}, \\text{&lt;unused2&gt;}, \\dots, \\text{&lt;unused5&gt;}\\}\\)</p> <p>These unused tokens are treated like regular query tokens during training but allow the model to adjust the importance of certain parts of the query during retrieval.</p> <p>I won\u2019t further explain query augmentation in detail, please refer ColBERT\u2019s paper for more detailed explanation: ColBERT paper</p>"},{"location":"blog/colpali-indepth-blog/#3-low-rank-adapters-lora-for-efficient-training","title":"3. Low-Rank Adapters (LoRA) for Efficient Training","text":"<p>Training large models from scratch is computationally expensive and often impractical. To make the training process more efficient, ColPali uses Low-Rank Adapters (LoRA), a technique that allows fine-tuning a small subset of the model\u2019s parameters without requiring the full model to be updated.</p> <p>How LoRA Works:</p> <p>In the transformer layers of the language model:</p> <ul> <li>LoRA adds a low-rank matrix to the attention weights and only fine-tunes these low-rank matrices during training.</li> <li>This drastically reduces the number of trainable parameters, making fine-tuning more efficient.</li> </ul> <p>Again, not going in-depth into LoRA here (it\u2019s out of scope of this blog)</p>"},{"location":"blog/colpali-indepth-blog/#bisiglip-and-bipali-embedding-techniques","title":"BiSigLIP and BiPali: Embedding Techniques","text":""},{"location":"blog/colpali-indepth-blog/#bisiglip-visual-textual-embedding-model","title":"BiSigLIP: Visual-Textual Embedding Model","text":"<p>ColPali builds on SigLIP, a vision-language bi-encoder model. SigLIP is pre-trained on WebLI, a massive corpus of billions of image-text pairs. In ColPali, SigLIP is fine-tuned on a document retrieval dataset, which allows it to handle the visual and textual elements of documents more effectively.</p> <ol> <li>SigLIP as a Bi-Encoder: SigLIP generates embeddings for both image patches and text and aligns them in the same latent space, enabling cross-modal comparison between text queries and document images.</li> <li>BiSigLIP Fine-Tuning: ColPali fine-tunes SigLIP on a document-oriented dataset to improve its retrieval performance.</li> </ol>"},{"location":"blog/colpali-indepth-blog/#bipali-pairing-image-patches-with-paligemma","title":"BiPali: Pairing Image Patches with PaliGemma","text":"<p>In the BiPali model, SigLIP-generated image patch embeddings are passed through a language model (PaliGemma) to obtain contextualized output embeddings. This enhances the embeddings by adding language-based context, which helps in tasks that require understanding the relationship between text and images.</p> <ul> <li>Pooling Operation: The image patch embeddings are average pooled to create a single dense vector representing the entire document. Remember, BiPali initially explored pooling as a way to create a single dense vector from image patch embeddings. But, ColPali does not use pooling.</li> </ul>"},{"location":"blog/colpali-indepth-blog/#late-interaction","title":"Late Interaction","text":"<p>The ColPali model further extends this by introducing token-level interaction between text and image embeddings, which drastically improves performance on complex visual tasks.</p> <p>As we near the conclusion of this blog, wouldn't you be curious to explore the outcomes of Colpali's experiments with various architectures and techniques? Let's dive in!</p>"},{"location":"blog/colpali-indepth-blog/#results-and-lessons-from-colpalis-iterative-development","title":"Results and Lessons from ColPali\u2019s Iterative Development","text":"<p>In constructing ColPali, the authors iteratively built and improved upon various models, starting with an off-the-shelf SigLIP model, followed by pairing SigLIP with a language model (PaliGemma) and finally adding Late Interaction (as seen above). Each iteration provided insights into the model's performance across different document retrieval tasks, particularly for documents with complex visual elements like tables and figures.</p>"},{"location":"blog/colpali-indepth-blog/#improvements-with-bisiglip","title":"Improvements with BiSigLIP","text":"<p>When fine-tuned on the document retrieval dataset, BiSigLIP showed significant improvements across various document retrieval tasks:</p> <ul> <li>ArxivQA (figure retrieval): Focused on retrieving figures from academic papers.</li> <li>TabFQuAD (table retrieval): Tasked with retrieving tables from documents.</li> </ul> <p>By further fine-tuning SigLIP\u2019s text and image encoders on this document-specific dataset, ColPali achieved improved performance for tasks requiring understanding both textual and visual information.</p>"},{"location":"blog/colpali-indepth-blog/#performance-of-bipali","title":"Performance of BiPali","text":"<p>After fine-tuning on the training dataset, BiPali showed a slight decrease in performance for English document retrieval tasks compared to BiSigLIP. This is likely due to the fact that PaliGemma was not originally trained for contrastive matching tasks, but rather for next token prediction. The authors' contrastive fine-tuning on 100K images was significantly smaller than SigLIP\u2019s original contrastive training, leading to weaker performance in English retrieval tasks.</p> <ul> <li> <p>Why BiPali Shines in Multilingual Tasks</p> <p>Despite the slight drop in English performance, BiPali showed notable improvements in French tasks, indicating that PaliGemma\u2019s multilingual pretraining helps with multilingual document retrieval. Interestingly, although the training dataset did not contain any non-English samples, PaliGemma\u2019s LLM (Gemma-2B) was able to handle multilingual data, resulting in better cross-lingual generalization.</p> </li> </ul>"},{"location":"blog/colpali-indepth-blog/#why-late-interaction-improves-performance","title":"Why Late Interaction Improves Performance","text":"<p>By focusing on the most relevant document patches for each query token, Late Interaction enables ColPali to excel in tasks that require detailed understanding of both text and visual elements. This was especially evident in more visually complex tasks, such as:</p> <ul> <li>InfographicVQA (infographic retrieval).</li> <li>ArxivQA (figure retrieval).</li> <li>TabFQuAD (table retrieval).</li> </ul> <p>ColPali outperformed baselines such as Unstructured and captioning-based models, as well as all evaluated text-image embedding models. This stark improvement in visually complex tasks demonstrates the power of Late Interaction in multimodal retrieval.</p>"},{"location":"blog/colpali-indepth-blog/#lessons-from-negative-results-colsiglip-and-bisiglip-with-late-interaction","title":"Lessons from Negative Results: ColSigLIP and BiSigLIP with Late Interaction","text":""},{"location":"blog/colpali-indepth-blog/#colsiglip","title":"ColSigLIP:","text":"<p>A version of BiSigLIP with Late Interaction (ColSigLIP) was also tested but performed poorly. The authors attribute this to the fact that in SigLIP\u2019s pre-training, only a pooled latent representation is used in the contrastive loss, which does not optimize individual patch and token embeddings as effectively as ColPali.</p>"},{"location":"blog/colpali-indepth-blog/#bisiglip-paligemma","title":"BiSigLIP + PaliGemma:","text":"<p>The authors also experimented with using the text representations from PaliGemma and the image representations from SigLIP. However, this variant also performed poorly, likely due to a misalignment between the SigLIP embeddings and the Gemma embeddings after PaliGemma fine-tuning.</p>"},{"location":"blog/colpali-indepth-blog/#querying-latencies-and-memory-footprint","title":"Querying Latencies and Memory Footprint","text":"<p>One of the challenges with Late Interaction is that it can increase querying latency. In online querying scenarios:</p> <ul> <li>BGE-M3 embedding model takes approximately 22 ms to encode a query with 15 tokens.</li> <li>In contrast, ColPali takes about 30 ms for query encoding due to the additional complexity introduced by the language model.</li> </ul> <p>However, for smaller document corpora, the overhead from Late Interaction is minimal\u2014approximately 1 ms per 1000 pages. This makes ColPali a scalable solution for document retrieval, even in larger datasets.</p>"},{"location":"blog/colpali-indepth-blog/#conclusion-building-colpalifrom-siglip-to-late-interaction","title":"Conclusion: Building ColPali\u2014From SigLIP to Late Interaction","text":"<p>The iterative construction of ColPali highlights how combining vision-language models (SigLIP), language models (PaliGemma), and Late Interaction led to a state-of-the-art document retrieval system. Each step added new capabilities:</p> <ul> <li>SigLIP provided strong bi-modal embeddings.</li> <li>PaliGemma enhanced these embeddings with language model context.</li> <li>Late Interaction enabled ColPali to excel in token-level retrieval, focusing on the most relevant parts of documents for each query.</li> </ul> <p>pheww! it was a long long blog! I really hope this blog can help in understanding about colpali in detail. cheers to all the readers for sticking till the end!</p>"},{"location":"blog/rag-best-practices/","title":"Retrieval Augmented Generation Best Practices","text":""},{"location":"blog/rag-best-practices/#retrieval-and-ranking-matter","title":"Retrieval and Ranking Matter!","text":""},{"location":"blog/rag-best-practices/#chunking","title":"Chunking","text":"<ol> <li>Including section title in your chunks improves that, so does keywords from the documents</li> <li>Different token-efficient separators in your chunks e.g. ### is a single token in GPT</li> </ol>"},{"location":"blog/rag-best-practices/#examples","title":"Examples","text":"<ol> <li>Few examples are better than no examples</li> <li>Examples at the start and end have the highest weight, the middle ones are kinda forgotten by the LLM</li> </ol>"},{"location":"blog/rag-best-practices/#re-rankers","title":"Re Rankers","text":"<p>Latency permitting \u2014 use a ReRanker \u2014 Cohere, Sentence Transformers and BGE have decent ones out of the box</p>"},{"location":"blog/rag-best-practices/#embedding","title":"Embedding","text":"<p>Use the right embedding for the right problem: </p> <p>GTE, BGE are best for most support, sales, and FAQ kind of applications. </p> <p>OpenAI is the easiest for Code Embedding to use. </p> <p>e5 family does outside English and Chinese</p> <p>If you can, finetune the embedding to your domain \u2014 takes about 20 minutes on a modern laptop or Colab notebook, improves recall by upto 30-50%</p>"},{"location":"blog/rag-best-practices/#evaluation","title":"Evaluation","text":"<p>Evaluation Driven Development makes your entire \"dev\" iteration much faster. </p> <p>Think of these as the \"running the code to see if it works\"</p> <p>Strongly recommend using Ragas for something like this. They've Langchain and Llama Index integrations too which are great for real world scenarios.</p>"},{"location":"blog/rag-best-practices/#scaling","title":"Scaling","text":""},{"location":"blog/rag-best-practices/#llm-reliability","title":"LLM Reliability","text":"<p>Have a failover LLM for when your primary LLM is down, slow or just not working well. Can you switch to a different LLM in 1 minute or less automatically?</p>"},{"location":"blog/rag-best-practices/#vector-store","title":"Vector Store","text":"<p>When you're hitting latency and throughput limits on the Vector Store, consider using scalar quantization with a dedicated vector store like Qdrant or Weaviate</p> <p>Qdrant also has Binary Quantization which allows you to scale 30-40x with OpenAI Embeddings.</p>"},{"location":"blog/rag-best-practices/#finetuning","title":"Finetuning","text":"<p>LLM: OpenAI GPT3.5 will often be as good as GPT4 with finetuning.</p> <p>Needs about 100 records and you get the 30% latency improvements for free</p> <p>So quite often worth the effort!</p> <p>This extends to OSS LLM models. Can't hurt to \"pretrain\" finetune your Mistral or Zephyr7B for $5</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/rag/","title":"RAG","text":""},{"location":"blog/category/production/","title":"production","text":""},{"location":"blog/category/machine-learning/","title":"machine-learning","text":""},{"location":"blog/category/colpali/","title":"ColPali","text":""},{"location":"blog/category/document-retrieval/","title":"Document Retrieval","text":""},{"location":"blog/category/quick-notes/","title":"Quick Notes","text":""}]}